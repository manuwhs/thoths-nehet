{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapying\n",
    "\n",
    "Since I could not find any reliable English dictionary online, I decided to scrap it from the most reliable sources. For this purpose the class XXXX has been created.\n",
    "\n",
    "This class uses the scrapers from the github repository:\n",
    "https://github.com/kiasar/Dictionary_crawler\n",
    "\n",
    "The closest thing I found to an open dictionary is the one from the project Gutemberg but it seems that it is not that clean and consistent. So the efforts of parsing it are way higher than for the scrapped one, provided I do not get blocked by scrapyng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import json\n",
    "%matplotlib qt \n",
    "\n",
    "from thothsnehet.dictionary_crawler import DictionaryCrawler\n",
    "\n",
    "from thothsnehet.utils.basic import merge_dictionaries, get_unique_words, get_all_text_from_definitions\n",
    "\n",
    "from traphing.utils import unwrap\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_path = \"./definitions_example/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the folder for the examples from the previous execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(storage_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create instance of the Crawler\n",
    "\n",
    "When we initialize it we indicate the directory where the already downloaded definitions are. Futher definitions will also be downloaded (scrapped) to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_source = \"oxford\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_crawler = DictionaryCrawler(storage_path, dictionary_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DictionaryCrawler>\tobject has children:\n",
      "    <str>\tstorage_path:\t/home/montoya/Desktop/VScode/thoths-nehe\n",
      "    <str>\tdictionary_source:\toxford\n",
      "    <str>\tcrawlers_path:\t/home/montoya/Desktop/VScode/thoths-nehe\n",
      "    <dict>\twords_dict\n",
      "\n",
      "  <dict>\twords_dict has children:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unwrap(dictionary_crawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Crawl a list of words\n",
    "\n",
    "The most basic functionality is to download a list of word definitions using a scrapy crawler. The method crawl_definitions() serves this purpose. In its basic functionality, it simply calls the crawler process from the command line, retuns the results of the process and writes the file in jason lines format if successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"Your mom\".split(\" \")\n",
    "filename = \"words.jl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, error, return_code = dictionary_crawler.crawl_definitions(words, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.lexico.com/en/definition/Your', 'https://www.lexico.com/en/definition/mom']\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-26 10:53:21 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: dictionary_crawler)\n",
      "2020-07-26 10:53:21 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.4 (default, Aug 13 2019, 20:35:49) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-5.3.0-40-generic-x86_64-with-debian-buster-sid\n",
      "2020-07-26 10:53:21 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-07-26 10:53:21 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'dictionary_crawler',\n",
      " 'CONCURRENT_REQUESTS': 512,\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'FEED_FORMAT': 'jl',\n",
      " 'FEED_URI': '/home/montoya/Desktop/VScode/thoths-nehet/notebooks/./definitions_example/words.jl',\n",
      " 'NEWSPIDER_MODULE': 'dictionary_crawler.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['dictionary_crawler.spiders']}\n",
      "2020-07-26 10:53:21 [scrapy.extensions.telnet] INFO: Telnet Password: df458b449f13e054\n",
      "2020-07-26 10:53:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-07-26 10:53:21 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-07-26 10:53:21 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-07-26 10:53:21 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-07-26 10:53:21 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-07-26 10:53:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-07-26 10:53:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-07-26 10:53:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lexico.com/robots.txt> (referer: None)\n",
      "2020-07-26 10:53:22 [scra\n"
     ]
    }
   ],
   "source": [
    "print(error[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can load downlothe scrapped definitions file, which originally has a jason lines format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mom': {'noun': [\"One's mother.\"]},\n",
       " 'your': {'possessive determiner': ['Belonging to or associated with the person or people that the speaker is addressing.',\n",
       "   'Belonging to or associated with any person in general.',\n",
       "   'Used to denote someone or something that is familiar or typical of its kind.',\n",
       "   'Used when addressing the holder of certain titles.']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions = dictionary_crawler.read_crawled_words(filename)\n",
    "definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the method is calling a process using scrapy to download the files. We can access the output by setting the verbose parameter to 1. \n",
    "\n",
    "Also the words parameter can be just a string of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'house', 'is', 'red']\n"
     ]
    }
   ],
   "source": [
    "process_results = dictionary_crawler.crawl_definitions(\"the house is red\", filename, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mom', 'your', 'red', 'is', 'the', 'house'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions = dictionary_crawler.read_crawled_words(filename)\n",
    "definitions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Internal dictionary of words\n",
    "\n",
    "The dictionary_crawler object also has the functionality of loading all the definition files in the storage folder and set them as the internal variable words_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_crawler.load_definitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mom', 'your', 'red', 'is', 'the', 'house'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_crawler.words_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property words contains the list of words in the words_dict. It is computed on spot when accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mom', 'your', 'red', 'is', 'the', 'house']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_crawler.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Advanced crawling options\n",
    "\n",
    "Now that we have a reference of the words we have, we can also start doing cool things. \n",
    "\n",
    "We can just download the words that we do not already have in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue']\n"
     ]
    }
   ],
   "source": [
    "process_results = dictionary_crawler.crawl_definitions(\"the house is blue\", filename, only_if_missing = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also add a given file to the crawded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_crawler.add_crawled_words(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mom', 'your', 'red', 'is', 'the', 'house', 'blue']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_crawler.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Parsing playground\n",
    "\n",
    "The basic functionality of parsing was not perfect so lets try to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thothsnehet.scrapers.dictionary_crawler.spiders.parsers import oxford_parser\n",
    "from scrapy.http import HtmlResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def oxford_parser(response):\n",
    "    definition_dict = {}\n",
    "\n",
    "    # Each type of meaning [noun, verb...] is in a  <section class=\"gramb\">\n",
    "    for grammar_word_type_section in response.xpath(\"//section[@class='gramb']\"):\n",
    "        try:\n",
    "            # The first <span class=\"pos\">noun</span> contains the type of word\n",
    "            part_of_speech = grammar_word_type_section.xpath(\".//span[@class='pos']/text()\").extract()[0]\n",
    "        except:\n",
    "            part_of_speech = False\n",
    "\n",
    "        if part_of_speech:\n",
    "            definition_dict[part_of_speech] = dict()\n",
    "\n",
    "        # For each meaning <div class=\"trg\"> Only ther first layer since if there are subdefinitions also have this label\n",
    "        for meaning_div in grammar_word_type_section.xpath(\"./div[@class='trg']\"):\n",
    "            try:\n",
    "                # The first <span class=\"iteration\">1 contains the type of word\n",
    "                # If empty it can be a sub definition or \n",
    "                meaning_index = meaning_div.xpath(\".//span[@class='iteration']/text()\").extract()[0]\n",
    "                meaning_subindex = meaning_div.xpath(\".//span[@class='iteration']/text()\").extract()[0]\n",
    "                \n",
    "            except:\n",
    "                meaning_index = False\n",
    "            print(meaning_index)\n",
    "            \n",
    "            def_list = meaning_div.xpath(\".//span[@class='ind']\").extract()\n",
    "            print (def_list)\n",
    "            if not def_list:\n",
    "                def_list = meaning_div.xpath(\".//div[@class='empty_sense']//div[@class='crossReference']\").extract()\n",
    "                \n",
    "            \n",
    "            def_list = [re.sub(r'<.*?>', \"\", i).strip() for i in def_list]\n",
    "            def_list = [i for i in def_list if i]\n",
    "\n",
    "            if def_list and part_of_speech and meaning_index:\n",
    "                def_list= [def_list[0]]\n",
    "                if meaning_index in definition_dict[part_of_speech]:\n",
    "                    definition_dict[part_of_speech][meaning_index] += def_list\n",
    "                else:\n",
    "                    definition_dict[part_of_speech][meaning_index] = def_list\n",
    "        \n",
    "    return definition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<section class=\"gramb\">\\n    <h3 class=\"ps pos\"><sp'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_oxford_file_path = \"../thothsnehet/scrapers/dictionary_crawler/spiders/html_format_oxford.html\"\n",
    "html_doc = open(example_oxford_file_path, \"r\").read().encode('utf-8')  \n",
    "html_doc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = HtmlResponse(url = \"\", status=200, headers=None, body=html_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = oxford_parser(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun\n"
     ]
    }
   ],
   "source": [
    "for key in dictionary:\n",
    "    print(key)\n",
    "    for key2 in dictionary[key]:\n",
    "        print(key2)\n",
    "        [print(\" \".join(x.replace(\"\\n\", \"\").split())) for x in dictionary[key][key2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recursive definition crawling\n",
    "\n",
    "Given a initial set of words, we would like to recursively also download the set of words in the defitions if we do not have them until we are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"table\"\n",
    "recursive_depth = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Recursion level 1/2-------\n",
      "Source word: table\n",
      "['table']\n",
      "--> 104/107 unknown to unique words in definitions\n",
      "--> 0 words we already know we cannot get the definition to\n",
      "['defined', 'games', 'meeting', 'memory', 'wood']\n",
      "['formal', ')', 'flat', '.', 'piece']\n",
      "['records', 'discussion', 'legs', 'writing', 'folding']\n",
      "['with', 'household', 'consideration', 'unique', 'food']\n",
      "['and', 'rectangular', 'faces', 'facts', 'quarter']\n",
      "['place', 'at', 'furniture', 'discussions', 'level']\n",
      "['be', 'vertical', 'that', 'key', 'molding']\n",
      "['postpone', 'one', 'present', '(', 'placed']\n",
      "['surface', 'restaurant', 'gem', 'or', 'provided']\n",
      "['held', 'dummy', 'columns', 'figures', 'group']\n",
      "['half', 'in', 'especially', 'collection', 'horizontal']\n",
      "['issue', 'as', 'used', 'stored', 'two']\n",
      "['purposes', 'on', 'set', 'working', 'settle']\n",
      "['hand', 'it', 'backgammon', 'eating', 'typically']\n",
      "['each', 'by', 'objects', 'seated', 'top']\n",
      "['dispute', 'slab', 'displayed', 'which', ',']\n",
      "['series', 'cornice', 'data', 'formally', 'a']\n",
      "['providing', 'cut', 'an', 'exposed', 'of']\n",
      "['such', 'more', 'systematically', 'for', 'bearing']\n",
      "['board', 'meal', 'can', 'inscription', 'stone']\n",
      "['to', 'may', 'forum', 'playing']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1450516f3fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_still_unknown_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary_crawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_definitions_recursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/VScode/thoths-nehet/thothsnehet/dictionary_crawler.py\u001b[0m in \u001b[0;36mcrawl_definitions_recursively\u001b[0;34m(self, word, recursive_depth)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# Crawl the words and add them to the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 process_results = self.crawl_definitions(\n\u001b[0;32m--> 113\u001b[0;31m                     unknown_words, word_downloads_filename, only_if_missing=True, verbose=1)\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_crawled_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_downloads_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VScode/thoths-nehet/thothsnehet/dictionary_crawler.py\u001b[0m in \u001b[0;36mcrawl_definitions\u001b[0;34m(self, words, filename, only_if_missing, n_max_words, verbose)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_subprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VScode/thoths-nehet/thothsnehet/utils/subprocess.py\u001b[0m in \u001b[0;36mcall_subprocess\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m      9\u001b[0m                             stdin=subprocess.PIPE)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1679\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python36/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_still_unknown_words = dictionary_crawler.crawl_definitions_recursively(word, recursive_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_still_unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_still_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = set(all_still_unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(all_still_unknown_words).difference(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the words are not in the dictionary after scrapping there are multiple options:\n",
    "- It is just a word that needs steamming. Like legs - leg. We should not steam from the beggining, like for exmaple with programming, but we should try.\n",
    "- It is a symbol or number\n",
    "- It is a noun. \n",
    "\n",
    "How should we handle these cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of feelings\n",
    "\n",
    "We load a file containing a list of the most common feelings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/feelings.txt\", \"r\")\n",
    "feelings_words = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(feelings_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the first level of definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_crawler.crawl_definitions(feelings_words, dictionary_source, \"feelings.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "definitions = dictionary_crawler.read_crawled_words(\"feelings.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dictionary = merge_dictionaries(definitions)\n",
    "words_dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
